import os
import yaml
import argparse
import datetime
import json
import pickle
import numpy as np
from tqdm import tqdm
from collections import defaultdict

import faiss
from search_expand import flat_multisearch
from utils import calculate_avg_precision, load_embeddings, create_new_pickle
import mlog

def short_hash(s, length=9):
    import hashlib
    return hashlib.sha256(s.encode()).hexdigest()[:length]

def join_constructor(loader, node):
    seq = loader.construct_sequence(node)
    return ''.join([str(i) for i in seq])

yaml.add_constructor('!join', join_constructor)

def load_config(config_file):
    with open(config_file, 'r') as file:
        return yaml.load(file, Loader=yaml.FullLoader)

def load_test_data(test_file):
    with open(test_file, 'rb') as f:
        return pickle.load(f)

def run_benchmark(args, config, logger: mlog.SimpleLogger):
    active_dataset_config = config['datasets'][args.dataset]
    embed_path = active_dataset_config['embed_path']
    pickle_path = active_dataset_config['pickle_path']

    embed_dict = create_new_pickle(embed_path, pickle_path)
    average_embeddings = embed_dict['average_embeddings']
    dim = average_embeddings.shape[1]

    # Create IVF Flat index
    quantizer = faiss.IndexFlatL2(dim)
    index = faiss.IndexIVFFlat(quantizer, dim, args.k)
    index.train(average_embeddings)
    index.add(average_embeddings)
    index.nprobe = args.nprobes  # Set nprobe to the specified value

    vec_to_img = embed_dict['vec_to_img']
    
    results = {}

    # Add metadata to results
    results['metadata'] = {
        'dataset': args.dataset,
        'test_file': args.test_file,
        'k': args.k,
        'p_k': args.p_k,
        'nprobes': args.nprobes,
        'timestamp': datetime.datetime.now().isoformat()
    }

    test_data = load_test_data(args.test_file)
    n_iterations = len(test_data['all_queries'])
    queries = test_data['all_queries']
    gt_data = test_data['img_ground_truth']

    with tqdm(total=n_iterations, desc="Benchmark Progress") as pbar:
        all_ap = []
        counters = []
        selectivities = []
        n_imgs = []
        
        for iteration in range(n_iterations):
            query = queries[iteration]
            features = {f'v_{i}': query[i] for i in range(len(query))}
            gt_images, gt_distances = gt_data[iteration]
            n_segments = len(query)

            if len(gt_images) < args.p_k:
                continue

            logger.info(f"-----Begin----- (n_segments: {n_segments})")
            ivf_images, _, stat_dict = flat_multisearch(index, features, args.p_k, vec_to_img, 
                                            max_search_radius=1e6, exclusive_matching=False,
                                            logger=logger)
            ivf_counter = stat_dict['counter']
            ivf_selectivity = stat_dict['estimated_selectivity']
            ivf_n_imgs = stat_dict['n_imgs']
            logger.info("-----End-----")

            ivf_ap = calculate_avg_precision(gt_images, ivf_images)

            all_ap.append(ivf_ap)
            counters.append(ivf_counter)
            selectivities.append(ivf_selectivity)
            n_imgs.append(ivf_n_imgs)

            pbar.update(1)

    # Calculate final scores
    results['raw'] = {
        'ap': all_ap,
        'counters': counters,
        'selectivities': selectivities,
        'n_imgs': n_imgs
    }

    results['summary'] = {}
    for key in results['raw']:
        results['summary'][key] = {
            'mean': float(np.mean(results['raw'][key])),
            'std': float(np.std(results['raw'][key]))
        }

    return results

def main(args, logger: mlog.SimpleLogger):
    config = load_config(args.config)
    if args.dataset not in config['datasets']:
        logger.error(f"Error: Dataset '{args.dataset}' not found in config file.")
        return

    results = run_benchmark(args, config, logger=logger)

    # Save results to JSON file with unique name
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = f'ivf_flat_{args.dataset}_k{args.k}_p{args.p_k}_nprobes{args.nprobes}_{args.test_file.split("/")[-1]}.json'

    with open(output_file, 'w') as f:
        json.dump(results, f, indent=4)

    print(f"Results saved to {output_file}")
    return results

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="IVF Flat index benchmark script")
    parser.add_argument('--config', required=True, help='Path to the configuration file')
    parser.add_argument('--dataset', required=True, help='Name of the dataset to use')
    parser.add_argument('--test_file', required=True, type=str, help='Path to the test file generated by generate_tests.py')
    parser.add_argument('--k', required=True, type=int, help='Number of clusters for IVF')
    parser.add_argument('--p_k', type=int, default=10, help='Precision at k setting (default: 10)')
    parser.add_argument('--nprobes', required=True, type=int, help='Number of probes for IVF')
    
    args = parser.parse_args()

    hash_name = vars(args)
    hash_name = ''.join([str(v) for v in hash_name.values()]) + str(datetime.datetime.now())
    hash_name = short_hash(hash_name, length=9)
    logger_name = f"benchmark_ivf_flat_{args.dataset}_{hash_name}.log"
    logger = mlog.SimpleLogger(name=logger_name, log_dir="logs", file_logging=True)
    main(args, logger)
