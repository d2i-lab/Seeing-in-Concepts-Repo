{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import hashlib\n",
    "from utils import create_new_pickle\n",
    "\n",
    "\n",
    "coco_path = '/data/users/jie/data-slicing/bdd100k/'\n",
    "embed_path = os.path.join(coco_path, 'embeds/sam_maskclip')\n",
    "# pickle_path = 'train2017_vitl_fixed_maskclip-fast.pkl'\n",
    "pickle_path = os.path.join(coco_path, 'embeds/bdd100k_train_maskclip-fast.pkl')\n",
    "embed_dict = create_new_pickle(embed_path, pickle_path)\n",
    "average_embeddings = embed_dict['average_embeddings']\n",
    "dim = average_embeddings.shape[1]\n",
    "k_coarse = 1023\n",
    "m = 256\n",
    "nbits = 8\n",
    "n_probes = 20\n",
    "\n",
    "train_sample_size = int(128 * math.sqrt(average_embeddings.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.627 GB\n"
     ]
    }
   ],
   "source": [
    "print(f\"{average_embeddings.nbytes / 1024 / 1024 / 1024:.3f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dict['embed_path'] = embed_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is hash 25ddeb1\n",
      "Loading cached index from cached_index_3f2f8d5845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING clustering 1023 points to 1023 centroids: please provide at least 39897 training points\n",
      "WARNING clustering 1023 points to 1023 centroids: please provide at least 39897 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans trained (hax) 1023\n",
      "Building Flat index\n",
      "Building IVF-Flat index\n",
      "This is training embeds (366384, 512)\n",
      "521 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "70000it [01:19, 885.25it/s] \n"
     ]
    }
   ],
   "source": [
    "import data\n",
    "\n",
    "index_pq, index_flat_cpu, index_ivf_flat_cpu, packd, img_concept_bitmap, all_images, pqq, kmeans = \\\n",
    "    data.get_indices(dim, k_coarse, m, nbits, n_probes, embed_dict, train_sample_size=train_sample_size,\n",
    "                     build_ivf_flat=True, cache_enabled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def improved_bitmap_to_pyroaring(bitmap):\n",
    "    \"\"\"\n",
    "    Convert a NumPy bitmap to PyRoaring bitmaps with careful verification.\n",
    "    \n",
    "    Args:\n",
    "        bitmap: A 2D NumPy boolean array\n",
    "    \n",
    "    Returns:\n",
    "        List of PyRoaring BitMap objects, one for each column\n",
    "    \"\"\"\n",
    "    from pyroaring import BitMap\n",
    "    import numpy as np\n",
    "    \n",
    "    _, n_cols = bitmap.shape\n",
    "    roarings = []\n",
    "    \n",
    "    for col in range(n_cols):\n",
    "        # Find indices where column is True\n",
    "        indices = np.where(bitmap[:, col])[0]\n",
    "        \n",
    "        # Create BitMap from indices\n",
    "        # Important: PyRoaring requires uint32 indices\n",
    "        # roaring = BitMap(indices.astype(np.uint32))\n",
    "        roaring = BitMap(indices)\n",
    "        # Verification step\n",
    "        original_count = np.sum(bitmap[:, col])\n",
    "        roaring_count = len(roaring)\n",
    "        assert original_count == roaring_count, f\"Column {col} conversion mismatch! Original: {original_count}, Roaring: {roaring_count}\"\n",
    "        \n",
    "        \n",
    "        roarings.append(roaring)\n",
    "    \n",
    "    return roarings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kmeans centroids: 1.998046875 MB\n",
      "packd: 2032.3006591796875 MB\n",
      "packd expected:  96.0 MB\n",
      "codebook: 0.5 MB\n",
      "bitmap: 68.29261779785156 MB\n",
      "total bitmap: 2.054 GB\n",
      "total sparse: 2.027 GB\n",
      "total roaring: 1.994 GB\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "from scipy import sparse\n",
    "from pympler import asizeof\n",
    "# our memory usage\n",
    "# * kmeans centroids\n",
    "# * quantizer codebook\n",
    "# * bitmap\n",
    "# * packd\n",
    "\n",
    "ksize_bytes = kmeans.centroids.shape[0] * kmeans.centroids.shape[1] * np.dtype(np.float32).itemsize\n",
    "packd_key_bytes = asizeof.asizeof(packd.keys)\n",
    "packd_value_bytes = len(average_embeddings) * m * nbits / 8\n",
    "packd_bytes = packd_key_bytes + packd_value_bytes\n",
    "codebook = faiss.vector_to_array(pqq.centroids).reshape(pqq.M, pqq.ksub, pqq.dsub)\n",
    "codebook_bytes = codebook.nbytes\n",
    "bitmap_bytes = img_concept_bitmap.nbytes\n",
    "sparse_mat_bytes = asizeof.asizeof(sparse.csr_matrix(img_concept_bitmap))\n",
    "roaring_bytes = asizeof.asizeof(improved_bitmap_to_pyroaring(img_concept_bitmap))\n",
    "\n",
    "print(f'kmeans centroids: {ksize_bytes / 1024 / 1024} MB')\n",
    "print(f'packd: {packd_bytes / 1024 / 1024} MB')\n",
    "print('packd expected: ', packd.memory_usage()['expected'] / 1024 / 1024, 'MB')\n",
    "print(f'codebook: {codebook_bytes / 1024 / 1024} MB')\n",
    "print(f'bitmap: {bitmap_bytes / 1024 / 1024} MB')\n",
    "\n",
    "us_total_bytes = ksize_bytes + packd_bytes + codebook_bytes + bitmap_bytes \n",
    "print(f'total bitmap: {us_total_bytes / 1024 / 1024 / 1024:.3f} GB')\n",
    "us_total_bytes_sparse = ksize_bytes + packd_bytes + codebook_bytes + sparse_mat_bytes\n",
    "print(f'total sparse: {us_total_bytes_sparse / 1024 / 1024 / 1024:.3f} GB')\n",
    "us_total_bytes_roaring = ksize_bytes + packd_bytes + codebook_bytes + roaring_bytes\n",
    "print(f'total roaring: {us_total_bytes_roaring / 1024 / 1024 / 1024:.3f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kmeans centroids: 1.998046875 MB\n",
      "inverted list: 2031.555233001709 MB\n",
      "codebook: 0.5 MB\n",
      "total: 1.986 GB\n"
     ]
    }
   ],
   "source": [
    "# PQ memory usage\n",
    "# centroids \n",
    "import numpy as np\n",
    "# codebook\n",
    "# inverted list\n",
    "import faiss.contrib.inspect_tools\n",
    "\n",
    "def get_invlist(invlists, l):\n",
    "    \"\"\" returns the inverted lists content. \"\"\"\n",
    "    ls = invlists.list_size(l)\n",
    "    list_ids = np.zeros(ls, dtype='int32') # Can be made 32\n",
    "    x = invlists.get_ids(l)\n",
    "    faiss.memcpy(faiss.swig_ptr(list_ids), x, list_ids.nbytes)\n",
    "    invlists.release_ids(l, x)\n",
    "    x = invlists.get_codes(l)\n",
    "    list_codes = np.zeros((ls, invlists.code_size), dtype='uint8')\n",
    "    faiss.memcpy(faiss.swig_ptr(list_codes), x, list_codes.nbytes)\n",
    "    invlists.release_codes(l, x)    \n",
    "    return list_ids, list_codes\n",
    "\n",
    "\n",
    "ksize_bytes = kmeans.centroids.shape[0] * kmeans.centroids.shape[1] * np.dtype(np.float32).itemsize\n",
    "codebook = faiss.contrib.inspect_tools.get_pq_centroids(index_pq.pq)\n",
    "codebook_bytes = codebook.nbytes\n",
    "list_bytes = 0\n",
    "for i in range(index_pq.invlists.nlist):\n",
    "    list_ids, list_codes = get_invlist(index_pq.invlists, i)\n",
    "    list_bytes += list_ids.nbytes + list_codes.nbytes\n",
    "    # We ignore list_ids because we have an equivalent mapping tracking the vector ids\n",
    "    # list_bytes +=  list_codes.nbytes\n",
    "\n",
    "print(f'kmeans centroids: {ksize_bytes / 1024 / 1024} MB')\n",
    "print(f'inverted list: {list_bytes / 1024 / 1024} MB')\n",
    "print(f'codebook: {codebook_bytes / 1024 / 1024} MB')\n",
    "pq_total_bytes = ksize_bytes + list_bytes + codebook_bytes\n",
    "print(f'total: {pq_total_bytes / 1024 / 1024 / 1024:.3f} GB')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6581531"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "packd.used - average_embeddings.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_list_ids[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.157777786254883"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_embeddings.nbytes / 1024 / 1024 / 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compression ratio us: 0.03767345892755861\n",
      "compression ratio pq: 0.0333592294720941\n"
     ]
    }
   ],
   "source": [
    "original =  (average_embeddings.nbytes)\n",
    "compression_ratio_us = us_total_bytes / original\n",
    "compression_ratio_pq = pq_total_bytes / original\n",
    "\n",
    "print(f'compression ratio us: {compression_ratio_us}')\n",
    "print(f'compression ratio pq: {compression_ratio_pq}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original|| 15.627347946166992 GB\n",
      "us|| 602.8659210205078 MB\n",
      "us_sparse|| 574.9774932861328 MB\n",
      "pq|| 533.8278770446777 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('original||', original / 1024 / 1024 / 1024, 'GB')\n",
    "print('us||', us_total_bytes / 1024 / 1024 , 'MB')\n",
    "print('us_sparse||', us_total_bytes_sparse / 1024 / 1024 , 'MB')\n",
    "print('pq||', pq_total_bytes / 1024 / 1024 , 'MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "424224"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_ids.nbytes + list_codes.nbytes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
